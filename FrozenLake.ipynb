{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random action 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/500 --------------------- \n",
      "--> Reward: 0.0, #Steps: 100,                 Avg Rewards: 0.0, Avg steps: 100.0\n",
      "Episode 1/500 --------------------- \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m truncated: \n\u001b[0;32m     64\u001b[0m     \u001b[39m# a = np.random.choice(NUM_ACTS, p=PI[s])\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     a \u001b[39m=\u001b[39m get_action(s)\n\u001b[1;32m---> 66\u001b[0m     next_s, r, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(a)       \n\u001b[0;32m     67\u001b[0m     traj\u001b[39m.\u001b[39mappend((s, a, r))\n\u001b[0;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m VERVOSE: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mState:\u001b[39m\u001b[39m\"\u001b[39m, s, \u001b[39m\"\u001b[39m\u001b[39mAction\u001b[39m\u001b[39m\"\u001b[39m, a, \u001b[39m\"\u001b[39m\u001b[39mReward:\u001b[39m\u001b[39m\"\u001b[39m, r, \u001b[39m\"\u001b[39m\u001b[39mTerminated:\u001b[39m\u001b[39m\"\u001b[39m, terminated, \u001b[39m\"\u001b[39m\u001b[39mTruncated:\u001b[39m\u001b[39m\"\u001b[39m, truncated, \u001b[39m\"\u001b[39m\u001b[39mInfo:\u001b[39m\u001b[39m\"\u001b[39m, info)\n",
      "File \u001b[1;32mC:\\Projects\\gym\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mC:\\Projects\\gym\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mC:\\Projects\\gym\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mC:\\Projects\\gym\\gym\\envs\\toy_text\\frozen_lake.py:252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[1;32mC:\\Projects\\gym\\gym\\envs\\toy_text\\frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32mC:\\Projects\\gym\\gym\\envs\\toy_text\\frozen_lake.py:373\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    371\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    372\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    374\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    376\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# On-policy first-visit MC control (p.101)\n",
    "\n",
    "# TODO \n",
    "# 1. performance graph\n",
    "\n",
    "# Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Map setting\n",
    "map_desc = [\"SFFF\", \"FHFF\", \"FFFH\", \"HFFG\"]\n",
    "# map_desc = [\"SFFF\", \"FFFH\", \"FFFF\", \"FFFG\"]\n",
    "env = gym.make('FrozenLake-v1', desc=map_desc, map_name=\"4x4\", is_slippery=False, render_mode='human')\n",
    "\n",
    "# Constants\n",
    "NUM_ACTS = env.action_space.n\n",
    "ETA = 0.1\n",
    "NUM_EPISODES = 500\n",
    "GAMMA = 0.9\n",
    "VERVOSE = False\n",
    "\n",
    "# Global vars\n",
    "Q = defaultdict(lambda: np.random.rand(NUM_ACTS))       # {s: [a,...,a]}\n",
    "Returns = defaultdict(list)                             # {(s, a): [r,...,r]}\n",
    "\n",
    "# Reporting\n",
    "tot_rewards = []\n",
    "tot_steps = []\n",
    "\n",
    "def random_argmax(value_list):\n",
    "  \"\"\" a random tie-breaking argmax \"\"\"\n",
    "  values = np.asarray(value_list)\n",
    "  return np.argmax(np.random.random(values.shape) * (values==values.max()))\n",
    "\n",
    "# get action with e-greedy\n",
    "def get_action(s):\n",
    "    a = -1\n",
    "    if np.random.rand() < ETA:\n",
    "        a = np.random.randint(NUM_ACTS)\n",
    "        if VERVOSE: print('Random action for %s -> %s' % (s, s))\n",
    "    else:\n",
    "        # Find a*\n",
    "        a = random_argmax(Q[s])\n",
    "        if VERVOSE: print('Greedy action for %s -> %s' % (s, s))\n",
    "    return a\n",
    "\n",
    "for e in range(NUM_EPISODES):\n",
    "    print(\"\\rEpisode {}/{} --------------------- \".format(e, NUM_EPISODES), end=\"\\n\")\n",
    "\n",
    "    # Env reset\n",
    "    s, info = env.reset()\n",
    "\n",
    "    # Terminiation condition\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Trajectory\n",
    "    traj = []\n",
    "    num_steps = 0\n",
    "    fin_reward = 0\n",
    "    # Generate an episode\n",
    "    while not terminated and not truncated: \n",
    "        # a = np.random.choice(NUM_ACTS, p=PI[s])\n",
    "        a = get_action(s)\n",
    "        next_s, r, terminated, truncated, info = env.step(a)       \n",
    "        traj.append((s, a, r))\n",
    "        if VERVOSE: print(\"State:\", s, \"Action\", a, \"Reward:\", r, \"Terminated:\", terminated, \"Truncated:\", truncated, \"Info:\", info)\n",
    "        s = next_s\n",
    "        num_steps += 1\n",
    "        fin_reward += r\n",
    "    tot_rewards.append(fin_reward)\n",
    "    tot_steps.append(num_steps)\n",
    "\n",
    "    print(f\"--> Reward: {fin_reward}, #Steps: {num_steps}, \\\n",
    "                Avg Rewards: {(np.average(tot_rewards))}, Avg steps: {np.average(tot_steps)}\")\n",
    "\n",
    "    # Check trajectory\n",
    "    if VERVOSE:\n",
    "        for i, t in enumerate(traj):\n",
    "            print(i, t)\n",
    "\n",
    "    # G\n",
    "    G = 0\n",
    "\n",
    "    # For each step\n",
    "    for i, (s, a, r) in reversed(list(enumerate(traj))):\n",
    "        # Update G\n",
    "        G = GAMMA*G + r\n",
    "        first_idx = next(j for j, t in enumerate(traj) if t[0] == s and t[1] == a)\n",
    "        if VERVOSE: print(i, (s, a, r), first_idx)\n",
    "        if i == first_idx:\n",
    "            # print(i, (s, a, r))\n",
    "            # Append G to (s, a)\n",
    "            Returns[(s, a)].append(G)\n",
    "            # Check returns\n",
    "            if VERVOSE: print('Returns(%s,%s) -> %s' % (s, a, Returns[(s, a)]))            \n",
    "            # Update Q\n",
    "            Q[s][a] = sum(Returns[(s, a)]) / len(Returns[(s, a)])\n",
    "            if VERVOSE: print('Update Q(%s,%s) -> %f' % (s, a, Q[s][a]))\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "'''\n",
    "Episode 99/100 ---------------------\n",
    "State: 0 Action 1 Reward: 0.0 Terminated: False Truncated: False Info: {'prob': 1.0}\n",
    "State: 4 Action 3 Reward: 0.0 Terminated: False Truncated: False Info: {'prob': 1.0}\n",
    "State: 0 Action 1 Reward: 0.0 Terminated: False Truncated: False Info: {'prob': 1.0}\n",
    "State: 4 Action 1 Reward: 0.0 Terminated: False Truncated: False Info: {'prob': 1.0}\n",
    "State: 8 Action 2 Reward: 0.0 Terminated: False Truncated: False Info: {'prob': 1.0}\n",
    "State: 9 Action 2 Reward: 0.0 Terminated: False Truncated: False Info: {'prob': 1.0}\n",
    "State: 10 Action 1 Reward: 0.0 Terminated: False Truncated: False Info: {'prob': 1.0}\n",
    "State: 14 Action 2 Reward: 1.0 Terminated: True Truncated: False Info: {'prob': 1.0}\n",
    "0 (0, 1, 0.0)\n",
    "1 (4, 3, 0.0)\n",
    "2 (0, 1, 0.0)\n",
    "3 (4, 1, 0.0)\n",
    "4 (8, 2, 0.0)\n",
    "5 (9, 2, 0.0)\n",
    "6 (10, 1, 0.0)\n",
    "7 (14, 2, 1.0)\n",
    "7 (14, 2, 1.0) 7\n",
    "Returns(14,2) -> [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "Update Q(14,2) -> 1.000000\n",
    "[0.7151168  0.72914016 1.         0.69932166] 2\n",
    "Update PI(14) -> [0.05 0.05 0.85 0.05]\n",
    "6 (10, 1, 0.0) 6\n",
    "Returns(10,1) -> [0.28242953648100017, 0.9, 0.9, 0.9, 0.9, 0.81, 0.9, 0.81, 0.9, 0.9, 0.9, 0.9, 0.7290000000000001, 0.9, 0.9, 0.9, 0.7290000000000001, 0.7290000000000001, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.7290000000000001, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.7290000000000001, 0.9, 0.7290000000000001, \n",
    "0.9, 0.9, 0.9, 0.38742048900000015, 0.9, 0.9, 0.9, 0.7290000000000001, 0.9]\n",
    "Update Q(10,1) -> 0.856015\n",
    "[0.         0.85601491 0.         0.3298725 ] 1\n",
    "Update PI(10) -> [0.05 0.85 0.05 0.05]\n",
    "5 (9, 2, 0.0) 5\n",
    "Returns(9,2) -> [0.0, 0.25418658283290013, 0.81, 0.81, 0.81, 0.81, 0.7290000000000001, 0.81, 0.7290000000000001, 0.81, 0.81, 0.81, 0.81, 0.6561000000000001, 0.81, 0.0, 0.81, 0.6561000000000001, 0.0, 0.0, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.81, 0.0, 0.81, 0.81, 0.6561000000000001, 0.81, 0.81, 0.81, 0.0, 0.5314410000000002, 0.81, 0.6561000000000001, 0.81, 0.81, 0.81, 0.81, 0.81, 0.6561000000000001, 0.81, 0.6561000000000001, 0.81, 0.34867844010000015, 0.81, 0.81, 0.6561000000000001, 0.81]\n",
    "Update Q(9,2) -> 0.680263\n",
    "[0.31047964 0.648      0.68026326 0.        ] 2\n",
    "Update PI(9) -> [0.05 0.05 0.85 0.05]\n",
    "4 (8, 2, 0.0) 4\n",
    "Returns(8,2) -> [0.0, 0.0, 0.0, 0.22876792454961012, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.5314410000000002, 0.7290000000000001, 0.6561000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.5904900000000002, 0.7290000000000001, 0.0, 0.7290000000000001, 0.47829690000000014, 0.0, 0.0, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.0, 0.7290000000000001, 0.7290000000000001, 0.5904900000000002, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.0, 0.38742048900000015, 0.7290000000000001, 0.5904900000000002, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.5904900000000002, 0.7290000000000001, 0.5904900000000002, 0.7290000000000001, 0.31381059609000017, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001, 0.5904900000000002, 0.7290000000000001]\n",
    "Update Q(8,2) -> 0.595012\n",
    "[0.50908121 0.         0.59501233 0.19683   ] 2\n",
    "Update PI(8) -> [0.05 0.05 0.85 0.05]\n",
    "3 (4, 1, 0.0) 3\n",
    "Returns(4,1) -> [0.0, 0.0, 0.0, 0.0, 0.0, 0.2058911320946491, 0.0, 0.6561000000000001, 0.0, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.47829690000000014, 0.6561000000000001, 0.5904900000000002, 0.6561000000000001, 0.5314410000000002, 0.6561000000000001, 0.6561000000000001, 0.5314410000000002, 0.6561000000000001, 0.0, 0.6561000000000001, 0.43046721000000016, 0.0, 0.0, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.0, 0.6561000000000001, 0.6561000000000001, 0.5904900000000002, 0.6561000000000001, 0.0, 0.6561000000000001, 0.6561000000000001, 0.5314410000000002, 0.5904900000000002, 0.0, 0.6561000000000001, 0.0, 0.5314410000000002, 0.0, 0.0, 0.31381059609000017, 0.5904900000000002, 0.5314410000000002, 0.6561000000000001, 0.0, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.6561000000000001, 0.5314410000000002, 0.6561000000000001, 0.5314410000000002, 0.6561000000000001, 0.28242953648100017, 0.6561000000000001, 0.6561000000000001, 0.5904900000000002, 0.5314410000000002, 0.6561000000000001]\n",
    "Update Q(4,1) -> 0.460980\n",
    "[0.20002849 0.46098019 0.         0.21964091] 1\n",
    "Update PI(4) -> [0.05 0.85 0.05 0.05]\n",
    "2 (0, 1, 0.0) 0\n",
    "1 (4, 3, 0.0) 1\n",
    "Returns(4,3) -> [0.0, 0.0, 0.0, 0.0, 0.16677181699666577, 0.0, 0.38742048900000015, 0.43046721000000016, 0.43046721000000016, 0.0, 0.5314410000000002, 0.47829690000000014, 0.43046721000000016, 0.5314410000000002]\n",
    "Update Q(4,3) -> 0.241912\n",
    "[0.20002849 0.46098019 0.         0.24191235] 1\n",
    "Update PI(4) -> [0.05 0.85 0.05 0.05]\n",
    "0 (0, 1, 0.0) 0\n",
    "Returns(0,1) -> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1500946352969992, 0.0, 0.0, 0.5904900000000002, 0.0, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.34867844010000015, 0.5904900000000002, 0.5314410000000002, 0.38742048900000015, 0.0, 0.38742048900000015, 0.5904900000000002, 0.5904900000000002, 0.43046721000000016, 0.5904900000000002, 0.0, \n",
    "0.5904900000000002, 0.0, 0.38742048900000015, 0.0, 0.0, 0.0, 0.47829690000000014, 0.43046721000000016, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.0, 0.0, 0.5904900000000002, 0.5904900000000002, 0.5314410000000002, 0.5904900000000002, 0.0, 0.5904900000000002, 0.5314410000000002, 0.47829690000000014, 0.5314410000000002, 0.0, 0.5904900000000002, 0.0, 0.47829690000000014, 0.0, 0.0, 0.0, 0.28242953648100017, 0.5314410000000002, 0.47829690000000014, 0.5904900000000002, 0.0, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.5904900000000002, 0.47829690000000014, 0.0, 0.5904900000000002, 0.0, 0.47829690000000014, 0.5904900000000002, 0.25418658283290013, 0.5904900000000002, 0.5904900000000002, 0.47829690000000014, 0.38742048900000015, 0.47829690000000014]\n",
    "Update Q(0,1) -> 0.347292\n",
    "[0.19367946 0.34729236 0.17780244 0.09356642] 1\n",
    "Update PI(0) -> [0.05 0.85 0.05 0.05]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# readchar이용 사람조정 버전\n",
    "- command에서 실행해야 조정가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import readchar\n",
    "\n",
    "#MACROS\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    " \n",
    "# Key mapping\n",
    "arrow_keys = {\n",
    "    'w' : UP,\n",
    "    's' : DOWN,\n",
    "    'd' : RIGHT,\n",
    "    'a' : LEFT\n",
    "}\n",
    " \n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='human')\n",
    "state = env.reset()\n",
    "\n",
    "while True:    \n",
    "    # Choose an action from keyboard\n",
    "    key = readchar.readkey()  \n",
    "    if key not in arrow_keys.keys():\n",
    "        \n",
    "        print(\"Game aborted!\")\n",
    "        break\n",
    " \n",
    "    action = arrow_keys[key]\n",
    "    # print(key, arrow_keys[key])\n",
    "    state, reward, done, truncated, info= env.step(action)       \n",
    "\n",
    "    env.render() # Show the board after action\n",
    "    print(\"State:\", state, \"Action\", action, \"Reward:\", reward, \"Info:\", info)\n",
    " \n",
    "    if done: \n",
    "        print(\"Finished with reward\", reward)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('rl_py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "275a6cec42f43076eac2a72a2f60dcdeff137cf5a0abd26cf35d1d7722567a54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
